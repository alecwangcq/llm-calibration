{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', torch_dtype='bfloat16', device_map='auto').eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Who was the first President of the United States?\n",
    "A) Abraham Lincoln\n",
    "B) George Washington\n",
    "C) Thomas Jefferson\n",
    "D) John Adams\n",
    "The answer is: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ĊĊ, Log-Prob: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load pre-trained LLaMA model and tokenizer\n",
    "# model_name = 'huggingface/llama-base'  # Example model name\n",
    "# model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text\n",
    "# input_text = \"Hello, how are you?\"\n",
    "# input_text=\"\"\"\n",
    "# Who was the first President of the United States?\n",
    "# A) Abraham Lincoln\n",
    "# B) George Washington\n",
    "# C) Thomas Jefferson\n",
    "# D) John Adams\n",
    "# The answer is: B) George Washington\n",
    "# \"\"\"\n",
    "\n",
    "all_inputs = []\n",
    "input_text='An integer c is a common divisor of two integers x and y if and only if c is a divisor of x and c is a divisor of y. Which of the following sets of integers could possibly be the set of all common divisors of two integers?\\nA. {-6,-2, -1, 1, 2, 6}\\nB. {-6, -2, -1, 0, 1, 2, 6}\\nC. {-6, -3, -2, -1, 1, 2, 3, 6}\\nD. {-6, -3, -2, -1, 0, 1, 2, 3, 6}\\nAnswer: '\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': f\"Please answer the following question with only A, B, C, D: {input_text}\"}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "all_inputs = [input_text ]\n",
    "\n",
    "input_text='Who was the first President of the United States?\\nA. Abraham Lincoln\\nB. George Washington\\nC. Thomas Jefferson\\nD. John Adams\\nAnswer: '\n",
    "\n",
    "# input_text += 'D'\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    [\n",
    "     {'role': 'user', 'content': f\"Please answer the question with only A, B, C, D: {input_text}\"}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "all_inputs.append(input_text)\n",
    "# input_text=\"\"\"\n",
    "# Multiple Choice Question: Who was the first President of the United States? \n",
    "# A) Abraham Lincoln\n",
    "# B) George Washington\n",
    "# C) Thomas Jefferson\n",
    "# D) John Adams\n",
    "# Answer: George Washington\n",
    "# \"\"\"\n",
    "# Tokenize input\n",
    "tokenizer.padding_side='left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# print(input_text)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, ).to(model.device)\n",
    "\n",
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Shift logits and labels for language modeling task\n",
    "shifted_logits = logits[..., :-1, :].contiguous()\n",
    "shifted_labels = inputs['input_ids'][..., 1:].contiguous()\n",
    "\n",
    "# Calculate log-probs\n",
    "log_probs = torch.nn.functional.log_softmax(shifted_logits, dim=-1)\n",
    "\n",
    "# Get the log-probs of the correct next token\n",
    "target_log_probs = log_probs.gather(-1, shifted_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# Convert back to text for inspection\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].int())\n",
    "\n",
    "# Print out log-probs for each token\n",
    "for token, log_prob in zip(decoded_tokens[1:][-1:], target_log_probs[0][-1:]):\n",
    "    print(f\"Token: {token}, Log-Prob: {log_prob.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 82])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.3125], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[..., -1, 426]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nPlease answer the question with only A, B, C, D: Who was the first President of the United States?\\nA. Abraham Lincoln\\nB. George Washington\\nC. Thomas Jefferson\\nD. John Adams\\nAnswer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "tensor([-0.7578], device='cuda:0')\n",
      "426\n",
      "tensor([11.3125], device='cuda:0')\n",
      "356\n",
      "tensor([0.0767], device='cuda:0')\n",
      "423\n",
      "tensor([2.4062], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for c in [' A', ' B', ' C', ' D']:\n",
    "    print(tokenizer(c)['input_ids'][-1])\n",
    "    print(logits[..., -1, tokenizer(c)['input_ids'][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "tensor([25.5000], device='cuda:0')\n",
      "33\n",
      "tensor([36.7500], device='cuda:0')\n",
      "34\n",
      "tensor([24.5000], device='cuda:0')\n",
      "35\n",
      "tensor([26.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for c in ['A', 'B', 'C', 'D']:\n",
    "    print(tokenizer(c)['input_ids'][-1])\n",
    "    print(logits[..., -1, tokenizer(c)['input_ids'][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3807260/3070696669.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(logits[..., -1, :][:, [362, 426, 356, 423]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5.9546e-06, 9.9991e-01, 7.3243e-06, 8.0927e-05]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(logits[..., -1, :][:, [362, 426, 356, 423]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9694580571201163\n",
      "0.0058245553131501095\n",
      "0.005713445455468459\n",
      "0.019003942111265108\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "log_probs = [\n",
    "    -0.06564091145992279,\n",
    "    -5.180295467376709,\n",
    "    -5.19955587387085,\n",
    "    -3.9977316856384277\n",
    "]\n",
    "\n",
    "for v in log_probs:\n",
    "    print(math.exp(v)/sum(math.exp(_) for _ in log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was the first President of the United States?\n",
      "A. Abraham Lincoln\n",
      "B. George Washington\n",
      "C. Thomas Jefferson\n",
      "D. John Adams\n",
      "Answer: \n",
      "torch.Size([1, 34, 128256])\n",
      "tensor([[128000,  15546,    574,    279,   1176,   4900,    315,    279,   3723,\n",
      "           4273,   5380,     32,     13,  37488,  25379,    198,     33,     13,\n",
      "          10058,   6652,    198,     34,     13,  11355,  34644,    198,     35,\n",
      "             13,   3842,  27329,    198,  16533,     25,    220]])\n",
      "<|begin_of_text|>Who was the first President of the United States?\n",
      "A. Abraham Lincoln\n",
      "B. George Washington\n",
      "C. Thomas Jefferson\n",
      "D. John Adams\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(input_text)\n",
    "print(logits.shape)\n",
    "print(inputs['input_ids'])\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was the first President of the United States?\n",
      "A. Abraham Lincoln\n",
      "B. George Washington\n",
      "C. Thomas Jefferson\n",
      "D. John Adams\n",
      "Answer: B\n",
      "torch.Size([1, 34, 128256])\n",
      "tensor([[128000,  15546,    574,    279,   1176,   4900,    315,    279,   3723,\n",
      "           4273,   5380,     32,     13,  37488,  25379,    198,     33,     13,\n",
      "          10058,   6652,    198,     34,     13,  11355,  34644,    198,     35,\n",
      "             13,   3842,  27329,    198,  16533,     25,    426]])\n",
      "<|begin_of_text|>Who was the first President of the United States?\n",
      "A. Abraham Lincoln\n",
      "B. George Washington\n",
      "C. Thomas Jefferson\n",
      "D. John Adams\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "print(input_text)\n",
    "print(logits.shape)\n",
    "print(inputs['input_ids'])\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 34, 128256])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.7291, -11.7851, -11.9531, -10.1250],\n",
       "        [-15.7022, -10.2309, -15.3585, -13.7300]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs[:, -2][..., torch.LongTensor([32, 33, 34, 35])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_log_probs = log_probs.gather(-1, shifted_labels.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 15339, 1917], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Who was the first President of the United States?\\nA. Abraham Lincoln\\nB. George Washington\\nC. Thomas Jefferson\\nD. John Adams\\nAnswer: '"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 34, 128256])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    prompt = df.iloc[idx, 0]\n",
    "    k = df.shape[1] - 2\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], df.iloc[idx, j+1])\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    if include_answer:\n",
    "        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(train_df, subject, k=-1):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(format_subject(subject))\n",
    "    if k == -1:\n",
    "        k = train_df.shape[0]\n",
    "    for i in range(k):\n",
    "        prompt += format_example(train_df, i)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('college_computer_science_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_end = format_example(test_df, 1, include_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = test_df.iloc[0, test_df.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An integer c is a common divisor of two integers x and y if and only if c is a divisor of x and c is a divisor of y. Which of the following sets of integers could possibly be the set of all common divisors of two integers?\\nA. {-6,-2, -1, 1, 2, 6}\\nB. {-6, -2, -1, 0, 1, 2, 6}\\nC. {-6, -3, -2, -1, 1, 2, 3, 6}\\nD. {-6, -3, -2, -1, 0, 1, 2, 3, 6}\\nAnswer: C\\n\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'Ċ',\n",
       " 'Who',\n",
       " 'Ġwas',\n",
       " 'Ġthe',\n",
       " 'Ġfirst',\n",
       " 'ĠPresident',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'ĠUnited',\n",
       " 'ĠStates',\n",
       " '?Ċ',\n",
       " 'A',\n",
       " ')',\n",
       " 'ĠAbraham',\n",
       " 'ĠLincoln',\n",
       " 'Ċ',\n",
       " 'B',\n",
       " ')',\n",
       " 'ĠGeorge',\n",
       " 'ĠWashington',\n",
       " 'Ċ',\n",
       " 'C',\n",
       " ')',\n",
       " 'ĠThomas',\n",
       " 'ĠJefferson',\n",
       " 'Ċ',\n",
       " 'D',\n",
       " ')',\n",
       " 'ĠJohn',\n",
       " 'ĠAdams',\n",
       " 'Ċ',\n",
       " 'The',\n",
       " 'Ġanswer',\n",
       " 'Ġis',\n",
       " ':',\n",
       " 'ĠĊ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Logits for each token in the sequence\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1674\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03mGenerates sequences of token ids for models with a language modeling head.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;124;03m            - [`~generation.GenerateBeamEncoderDecoderOutput`]\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1675\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1172\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[1;32m   1171\u001b[0m     exception_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[0;31mTypeError\u001b[0m: The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, return_dict_in_generate=True)\n",
    "    # Logits for each token in the sequence\n",
    "    logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/chaoqiw/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-02 21:02:33 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 21:02:46.869361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 21:02:51.556383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-10-02 21:03:01,012\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max_tokens must be at least 1, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[0;32m----> 3\u001b[0m gen_params \u001b[38;5;241m=\u001b[39m \u001b[43mSamplingParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Deterministic output\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We don't want to generate new tokens, just evaluate the inputs\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Request log probabilities\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/vllm/sampling_params.py:261\u001b[0m, in \u001b[0;36mSamplingParams.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_stop_str_in_output:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_text_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_beam_search:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_beam_search()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/vllm/sampling_params.py:306\u001b[0m, in \u001b[0;36mSamplingParams._verify_args\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_p must be in [0, 1], got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens must be at least 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_tokens \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_tokens must be greater than or equal to 0, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: max_tokens must be at least 1, got 0."
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "gen_params = SamplingParams(\n",
    "    temperature=1.0,  # Deterministic output\n",
    "    top_p=1.0,\n",
    "    max_tokens=1,  # We don't want to generate new tokens, just evaluate the inputs\n",
    "    logprobs=True,  # Request log probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm\n",
    "import os\n",
    "\n",
    "# You can replace this with the path to a locally saved model or a Hugging Face model\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"  # Example model (replace with appropriate model)\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load the LLaMA model with vllm.\"\"\"\n",
    "    # Create a LLaMA model instance\n",
    "    model = vllm.LLM(model_name)\n",
    "    return model\n",
    "\n",
    "def generate_response(model, prompt):\n",
    "    \"\"\"Generate a response from the LLaMA model given the input prompt.\"\"\"\n",
    "    output = model.generate(prompt, max_tokens=150)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vllm.LLM(MODEL_NAME, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Load the model\n",
    "    print(f\"Loading model '{MODEL_NAME}'...\")\n",
    "    model = load_model(MODEL_NAME)\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Begin the conversation\n",
    "    print(\"\\n--- Chat with LLaMA ---\")\n",
    "    print(\"Type 'exit' to end the chat.\")\n",
    "\n",
    "    while True:\n",
    "        # User input\n",
    "        prompt = input(\"\\nYou: \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            print(\"Ending chat.\")\n",
    "            break\n",
    "        \n",
    "        # Generate response\n",
    "        print(\"LLaMA is thinking...\")\n",
    "        response = generate_response(model, prompt)\n",
    "        print(f\"LLaMA: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
